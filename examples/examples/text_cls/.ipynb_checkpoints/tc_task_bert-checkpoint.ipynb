{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a3dd5a7-9e59-46b6-8c21-e5ddff0ecd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "260d701b-15f3-4417-9bb4-d3da32bd0ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dd_nlp_arsenal.dataset.sentence_cls_bert_dataset import SentenceBertClsDataset\n",
    "from dd_nlp_arsenal.processor.tokenizer.transformer import SentenceTokenizer\n",
    "from dd_nlp_arsenal.factory.task.cls_task.sentence_cls_task import SentenceCLSTask\n",
    "from dd_nlp_arsenal.model.text_cls.bert_model import bert_model\n",
    "\n",
    "from transformers import AdamW\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c29e01e5-592f-4b8a-a4a4-883624b32940",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SentenceTokenizer('/Users/anulz/github/code/NLP/PTM/bert-base-chinese',30)\n",
    "\n",
    "dataset = SentenceBertClsDataset('examples/text_cls/test.csv')\n",
    "dataset.convert_to_ids(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "642ec923-5ef4-43ad-a4e0-e9cb489a924a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    seed = 2022\n",
    "    max_sen_len = 30\n",
    "    train_batch_size = 8\n",
    "    val_batch_size = 8\n",
    "    bert_pretrained_name = '/Users/anulz/github/code/NLP/PTM/bert-base-chinese'\n",
    "    pre_model_type = 'bert-base-chinese'\n",
    "    n_classes = 2\n",
    "    is_use_fgm = False\n",
    "    is_use_focalloss = True\n",
    "    multi_gpu = False\n",
    "    ema_decay = 0.1\n",
    "    params_path = './'\n",
    "    num_workers = 0\n",
    "    n_epoch = 2\n",
    "    min_store_epoch = 1\n",
    "    is_use_rdrop = False\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fe4295d-0cab-4f44-a174-c7cefff9e317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/anulz/github/code/NLP/PTM/bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = bert_model(config)\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "884fa6b9-ca66-48d6-b6ef-92049c8251d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model type: bert-base-chinese\n",
      "device: cpu\n",
      "Init pre-train model...\n"
     ]
    }
   ],
   "source": [
    "task = SentenceCLSTask(model, optimizer, loss_func, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3cdd5ee-636e-4b1d-8b31-32bb2cddc7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "----------\n",
      "100%|██████████| 113/113 [04:21<00:00,  2.31s/it]\n",
      "Train loss 1.4357070384827335 train acc 0.5060908084163898\n",
      "Val   loss 0.7650013978664691   val acc 0.5247524752475248\n",
      "saving best_model_state val loss is 0.7650013978664691...\n",
      "Epoch 2/2\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save at checkpoints/bert_model_0407_14:13:27.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [04:16<00:00,  2.27s/it]\n",
      "Train loss 0.7283917140644208 train acc 0.4950166112956811\n",
      "Val   loss 0.7042246460914612   val acc 0.4752475247524752\n"
     ]
    }
   ],
   "source": [
    "task.fit(dataset,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809be737-4353-4dcd-a5d8-22fb9bb7b82a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
